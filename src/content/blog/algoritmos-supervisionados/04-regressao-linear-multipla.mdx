---
title: 'Regressão Linear Múltipla'
pubDate: 2025-12-20
description: 'Regressão Linear Múltipla'
heroImage: '/images/regressao_multipla.png'
---
import BlogImage from '../../../components/BlogImage.astro'

**Regressão Linear Múltipla** é simplesmente uma Regressão Linear com **múltiplas features (entradas)**, que difere da Regressão Linear Simples (ou univariada) no quesito entradas, onde vimos num [blog anterior](../algoritmos-supervisionados/02-regressao-linear-funcao-custo.mdx) que ela recebe apenas uma feature. Porém, a Regressão Linear Múltipla permanece buscando **apenas uma saída (target)**.

Como agora temos uma lista de features (ex.: tamanho da casa, número de quartos, número de andares e idade da casa), temos que atualizar nossos termos matemáticos.

* **Importante:** Não confunda **Regressão Linear Múltipla** com **Regressão Linear Multivariada**. Embora pareça lógico que o oposto de "univariada" seja "multivariada", a Regressão Multivariada é um modelo diferente que é usado quando queremos prever mais de uma saída (target) ao mesmo tempo. Aqui, focaremos apenas em prever um único valor (y) com base em várias características.

## Nova notação matemática

* $n$ **(minúsculo)** : Número total de features.
* $x_j$ **(x subscrito j)** : Representa a lista de features, onde $j$ vai de 1 até $n$.
* $x^{(i)}$ **(x sobrescrito i)** : É o vetor (lista de números) que contém todas as features do exemplo de treinamento número $i$.
* $x_{j}^{(i)}$ : É o valor específico $j$ do exemplo de treinamento $i$.

Para ficar mais claro, veja o exemplo abaixo:

<BlogImage
  src="/images/exemplo_regressao_linear_multipla.png"
  alt="Tabela de dados com 4 exemplos de treinamento de imóveis, mostrando colunas de features (x) e target (y)"
  caption="Uma tabela com exemplos de treino para o modelo que contém: uma coluna de index, quatro colunas de features e uma de target."
/>

Para consolidar essas notações, vamos analisar a figura acima com mais detalhes:

### 1. O número de features ($n$)
O $n$ representa a quantidade de **colunas de entradas** (as características do imóvel).
* **Na tabela:** Conte as colunas azuis rotuladas como $x$. Temos Tamanho ($x_1$), Quartos ($x_2$), Andares ($x_3$) e Idade ($x_4$).
* **Conclusão:** $n = 4$.

### 2. A lista de features ($x_j$)
Esta notação refere-se a uma **característica específica** (uma coluna inteira). O índice $j$ diz qual característica específica estamos olhando.
* **Exemplo:** $x_2$ representa a feature "Quartos". Se falarmos de $x_3$, estamos falando da característica "Andares".
* É a lista de todas as features disponíveis para o modelo aprender ($x_1$, $x_2$, $x_3$ e $x_4$).

### 3. O exemplo de treinamento ($x^{(i)}$)
O $x^{(i)}$ refere-se a uma **linha inteira** de dados, exceto pelo índice e o preço. É o conjunto de dados em uma única casa.
* **Exemplo:** Se olharmos na segunda casa ($i = 2$), o vetor de características é:
$$x^{(2)} = [1416, 3, 2, 40]$$ 
*(Isso significa: 1416 sqft, 3 quartos, 2 andares, 40 anos).*

### 4. O valor específico ($x_j^{(i)}$)
Aqui é onde cruzamos a linha e a coluna. O índice $i$ escolhe a **linha (casa)** e o índice $j$ escolhe a **coluna (feature)**.
* **Exemplo:** Queremos saber o número de andares ($j=3$) da terceira casa ($i=3$).
  * Olhamos na linha 3, coluna $x_3$.
  * Resultado: $x_3^{(3)} = 2$.

## O Novo Modelo matemático

Na Regressão Linear univariada tínhamos apenas uma feature, então a fórmula era simples: 

$$
f_{w,b}(x) = wx + b
$$

Agora que temos $n$ features, o modelo precisa somar a influência de cada uma delas. Então temos a nova fórmula:

$$
f_{\vec{w},b}(x) = w_1x_1 + w_2x_2 + w_3x_3 + \dots + w_nx_n + b
$$

### Vamos tentar interpretar essa fórmula na vida real

Para relembrarmos:
* **Viés (b):** Digamos que queremos prever o preço de uma casa (mais uma vez usando casa). Iniciaremos com $b = 80$ sendo o preço base da casa (R\$80k) independente do tamanho da casa ou da quantidade de quartos.
* **Peso do Tamanho ($w_1$):** Se $$w_1 = R\$0.1$$, significa que para cada pé quadrado extra, o valor sobe R\$0.1k (R\$100).
* **Peso da Idade ($w_4$):** Se $$w_4 = -R\$2$$ (negativo), significa que para cada ano que a casa envelhece, ela perde R\$2k de valor.

O modelo tenta encontrar os pesos certos que equilibrem a equação, levando a melhor previsão.

Podemos também transformar essa fórmula em algo mais bonito. Vimos que o modelo pode receber $n$ features e entendemos que toda a soma delas influenciam no resultado do modelo, então podemos dizer que teremos um somatório. Veja:

$$
f_{w,b}(x) = \sum_{j=1}^{n}w_jx_j+b
$$

É a mesma coisa que vimos anteriormente, porém numa notação mais elegante de somatório.

## Introdução à Notação Vetorial

Você pode ter percebido que dependendo da quantidade de features, a fórmula pode ficar gigantesca. Seria cansativo escrevê-la. Claro, vimos que podemos desenhar a fórmula como um somatório e deixá-la visualmente menor. Mas, computacionalmente, ainda seria um cálculo gigante necessitando de muitos loops. Então, para nos salvar, entra aqui a Álgebra Linear.

Podemos agrupar todos os parâmetros $w$ e todas as features $x$ em **Vetores**:

$$
\vec{w} = [w_1, w_2, w_3, \dots, w_n]
$$
$$
\vec{x} = [x_1, x_2, x_3, \dots, x_n]
$$

Agora que temos os parâmetros como vetores, podemos realizar a operação de **Produto Escalar** (que realiza a multiplicação dos pares correspondentes e soma todos os resultados), diminuindo a complexidade matemática do modelo e tornando-o mais rápido.

$$
f_{w,b}(x) = \vec{w} \cdot \vec{x} + b
$$


