---
title: 'Regressão Linear e Função de Custo'
pubDate: 2025-12-20
description: 'Minhas anotações sobre Regressão'
---
import BlogImage from '../../components/BlogImage.astro';

Regressão Linear, como pudemos ver na explicação teórica (caso não tenha visto, [acesse aqui](/blog/01-intro-machine-learning)), é um modelo de Aprendizado Supervisionado. O objetivo dele é ajustar uma linha que melhor se encaixe nos dados. Ou seja, estamos falando dos dados graficamente agora e isso nos dá até uma melhor visualização mental da quantificação desses dados.

## 1. O modelo

A função matemática que faz a previsão é a equação de uma reta (por isso uma linha). A mesma do ensino médio, mas ajustada para o Aprendizado de Máquina.

$$f_{{w,b}}(x) = wx + b$$

* **$x$**: É o dado de entrada.
* **$w$** (weight/peso): É quem determina a inclinação da reta.
* **$b$** (bias/viés): Determina onde a reta cruza o eixo Y.
* **$f_{{ w , b }}(x)$**: É a previsão da máquina ($\hat{y}$).

<BlogImage
  src="/public/images/regressao-linear.png"
  alt="Gráfico de dispersão com reta ajustada"
  caption="Ajustando uma reta aos dados de treino. O objetivo é fazer a reta passar o mais perto possível de todos os pontos."
/>

## 2. A função de Custo
A função de custo vem para nos dar uma métrica de qual ruim o nosso modelo está. Ou seja, se o modelo está errando muito a função de custo nos dará um valor alto e se o modelo estiver errando pouco a função de custo nos dará um valor baixo (perceba que não é uma métrica de quão bom ele é, mas sim de quanto menos ele erra).

$$J(w,b) = \frac{1}{2m} \sum_{i=1}^{m} \left(\hat{y}^{(i)} - y^{(i)}\right)^2$$

onde:
* **$i$**: Não falei o que era anteriormente, mas não há segredo. O $i$ representa qualquer dado que exista para o modelo. seja o 1º, o 46º, 127º, ou... o $i$-ésimo!
* **$\hat{y}$**: É aquele valor que o modelo previu. Veja que é diferente do valor real ($y$).
* **$(\hat{y}^{(i)}-{y}^{(i)})^2$**: Aqui temos a subtração do valor real com o valor previsto pelo modelo. É elevado ao quadrado para "inflar" o erro. Isso pune erros muitos altos (erro: 1000, eleve ao quadrado e veja o monstro). **Importante: Note que $\hat{y} = f_{{ w , b }}(x^{(i)})$**
* **$\sum_{i=1}^{m}$**: É o somatório de todos os erros. Do primeiro (note que o $i$ é igual a 1, ou seja, o primeiro dado. Se você ainda não entendeu o que é $i$, talvez isso tenha clareado) ao último dado que o modelo recebe, representado por $m$.
* **$$\frac{1}{2m}$$**: Perceba que estamos dividindo a estimativa de erro por $2m$. Isso permite que o erro seja estipulado pela média dos erros. Assim, não importa se foram muitos ou poucos dados utilizados, teremos uma estimativa justa. O 2 que acompanha $m$ é apenas para facilitar os cálculos para o computador quando envolver gradiente descendente.
* **$$J{(w,b)}$$**: Assim como $f(x)$ está para a própria função, $J{(w,b)}$ está para a função de custo.